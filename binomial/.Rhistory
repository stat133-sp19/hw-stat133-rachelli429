str_detect(tbl$Mark[1], "[0-9]\\.[0-9][0-9]")
str_locate(tbl$Mark[1], "[0-9]\\.[0-9][0-9]")
str_match(tbl$Mark[1], "[0-9]\\.[0-9][0-9]")
str_match(tbl$Mark[1:2], "[0-9]\\.[0-9][0-9]")
str_extract(tbl$Mark[1:2], "[0-9]\\.[0-9][0-9]")
# extract is what we want to use
str_extract(tbl$Mark, "[0-9]\\.[0-9][0-9]")
typeof(str_extract(tbl$Mark, "[0-9]\\.[0-9][0-9]"))
as.numeric(str_extract(tbl$Mark, "[0-9]\\.[0-9][0-9]"))
# extract first 4 characters of "Mark" column
str_detect(tbl$Mark[1], "[0-9]\\.[0-9][0-9]")
str_locate(tbl$Mark[1], "[0-9]\\.[0-9][0-9]")
str_match(tbl$Mark[1], "[0-9]\\.[0-9][0-9]")
str_match(tbl$Mark[1:2], "[0-9]\\.[0-9][0-9]")
str_extract(tbl$Mark[1:2], "[0-9]\\.[0-9][0-9]")
# extract is what we want to use
str_extract(tbl$Mark, "[0-9]\\.[0-9][0-9]")
typeof(str_extract(tbl$Mark, "[0-9]\\.[0-9][0-9]"))
as.numeric(str_extract(tbl$Mark, "[0-9]\\.[0-9][0-9]"))
tbl$Mark[1:5]
str_sub(tbl$Mark[1], start = 1, end = 4)
# allows you to extract part of a string based on position
str_sub(tbl$Mark[1], start = 1, end = 4)
str_sub(tbl$Mark, start = 1, end = 4)
peter
# athletes column
# separate name of athlete + country of origin
peter <- tbl$Athlete[1]
peter
tbl$Athlete[1:5]
string_extract(peter, "[A-Z][A-Z][A-Z]")
str_extract(peter, "[A-Z][A-Z][A-Z]")
str_extract(tbl$Athlete, "[A-Z][A-Z][A-Z]")
str_sub(peter, start = nchar(peter) - 5, end = nchar(peter))
str_sub(peter, start = nchar(peter) - 4, end = nchar(peter))
str_sub(peter, start = nchar(peter) - 3, end = nchar(peter) - 1)
str_sub(tbl$Athlete, start = nchar(tbl$Athlete) - 3, end = nchar(tbl$Athlete) - 1)
str_extract(peter, "\\w")
str_extract(peter, "\\w+")
str_extract(tbl$Athlete[1:5], "\\w+")
# w: matches a word character, only matches 1
# +: match 1 or more times
str_extract(peter, "\\w+")
str_extract(tbl$Athlete[1:5], "\\w+")
str_extract(peter, "\\w+ ")
str_extract(peter, "\\w+ \\w+")
str_extract(tbl$Athlete, "\\w+ \\w+")
str_extract(peter, "\\w+ \\w+")
str_extract(tbl$Athlete, "\\w+ \\w+")
str_sub(peter, start = 1, end = nchar(peter) - 3)
str_sub(peter, start = 1, end = nchar(peter) - 6)
str_sub(tbl$Athlete, start = 1, end = nchar(tbl$Athlete) - 6)
tbl$Date[1:5]
str_sub(tbl$Date, start = 1, end = nchar(tbl$Date) - 3)
str_replace(tbl$Date, pattern = "\\[[0-9]\\]", replacement = "")
shiny::runApp('Desktop/stat-133/hw-stat133/workout02')
knitr::opts_chunk$set(echo = TRUE)
github <- "https://raw.githubusercontent.com/gastonstat/r4strings"
textfile <- "/master/data/logfile.txt"
download.file(url = paste0(github, textfile), destfile = "logfile.txt")
logs <- readLines('logfile.txt')
sublogs <- logs[s]
set.seed(98765)
s <- sample(1:length(logs), size = 50)
sublogs <- logs[s]
sublogs
head(sublogs)
str_detect(sublogs, "jpg")
library(stringr)
library(rvest)
wiki_jump <- 'https://en.wikipedia.org/wiki/Men%27s_long_jump_world_record_progression'
long_jump <- read_html(wiki_jump)
tbl <- html_table(html_node(long_jump, 'table'))
str_detect(sublogs, "jpg")
str_detect(sublogs, "\.jpg")
str_detect(sublogs, "\\.jpg")
str_detect(sublongs[1:5], "\\.jpg")
str_detect(sublogs[1:5], "\\.jpg")
str_view(sublongs[1:5], "\\.jpg")
str_detect(sublogs[1:5], "\\.jpg")
str_view(sublogs[1:5], "\\.jpg")
# ".jpg" part of text gets highlighted (light gray box)
str_view(sublogs[1:10], "\\.jpg")
str_view(sublogs[1:10], "\\.[jg][pi][gf]")
str_detect(sublogs[1:10], "\\.[jg][pi][gf]")
# match .jpg and .gif files
str_view(sublogs[1:10], "\\.[jg][pi][gf]")
str_detect(sublogs[1:10], "\\.[jg][pi][gf]")
# match .jpg and .gif files
str_view(sublogs[1:10], "\\.[jgp][pin][gfg]")
str_detect(sublogs[1:10], "\\.[jgp][pin][gf]")
str_detect(sublogs[1:20], "\\.[jgp][pin][gf]")
str_view(sublogs[2:10], "\\.[jgp][pin][gfg]")
str_view(sublogs[1:20], "\\.[jgp][pin][gfg]")
# match .jpg, .gif, and .png files
str_view(sublogs[1:20], "\\.[jgp][pin][gfg]")
str_detect(sublogs[1:20], "\\.[jgp][pin][gf]")
# match .jpg, .gif, and .png files
str_view(sublogs[1:10], "\\.[jgp][pin][gfg]")
str_detect(sublogs[1:10], "\\.[jgp][pin][gf]")
str_extract(sublogs[1:10], "\\.[jgp][pin][gf]")
# match .jpg, .gif, and .png files
str_view(sublogs[1:10], "\\.[jgp][pin][gfg]")
str_detect(sublogs[1:10], "\\.[jgp][pin][gf]")
str_extract(sublogs[1:10], "\\.[jgp][pin][gf]")
str_extract(sublogs, "\\.[jgpi][pinc][gfo]")
# problem: catches .inf
# vertical bar | means or
str_extract(sublogs, "\\.jpg|\\.png|\\.gif|\\.ico")
str_extract(sublogs, "\\.(jpg|png|gif|ico")
str_extract(sublogs, "\\.(jpg|png|gif|ico)")
str_detect(sublogs, "\\.(jpg|png|gif|ico)")
sum(str_detect(sublogs, "\\.(jpg|png|gif|ico)"))
str_extract(sublogs, "/")
str_extract(sublogs, "/.\\.jpg")
str_extract(sublogs, "/archi\\.jpg")
str_extract(sublogs, "/\\w+\\.jpg")
str_extract(sublogs, "/\\w+\\.(gif|jpg|png|ico)")
str_extract(sublogs, "\\w+\\.(gif|jpg|png|ico)")
str_extract(sublogs, "[0-9][0-9]/[A-Z][a-z][a-z]/[0-9][0-9][0-9][0-9]")
str_extract(sublogs, "[0-9]+/[A-Z][a-z][a-z]/[0-9][0-9][0-9][0-9]")
str_extract(sublogs, "[0-9]+/[A-Z][a-z]+/[0-9][0-9][0-9][0-9]")
str_extract(sublogs, "[0-9]+/[A-Z][a-z]+/[0-9]+")
str_extract(sublogs, "[0-9]+/[A-Z][a-z]+/[0-9]{1,4}")
str_extract(sublogs, "[0-9]{1,2}/[A-Z][a-z]{1,2}/[0-9]{1,4}")
str_extract(sublogs, "[0-9]{2}/[A-Z][a-z]+/[0-9]{4}")
str_extract(sublogs, "[0-9]{2}:[0-9}{2}:[0-9]{2}")
str_extract(sublogs, "[0-9]{2}:[0-9]{2}:[0-9]{2}")
curl -O "https://raw.githubusercontent.com/ucb-stat133/stat133-labs/master/data/text-emotion.csv"
curl -O https://raw.githubusercontent.com/ucb-stat133/stat133-labs/master/data/text-emotion.csv
?curl
??curl
download.file("https://raw.githubusercontent.com/ucb-stat133/stat133-labs/master/data/text-emotion.csv")
download.file(destfile = "/", "https://raw.githubusercontent.com/ucb-stat133/stat133-labs/master/data/text-emotion.csv")
getwd()
download.file("/Users/rachelli/Desktop/stat-133", "https://raw.githubusercontent.com/ucb-stat133/stat133-labs/master/data/text-emotion.csv")
download.file("https://raw.githubusercontent.com/ucb-stat133/stat133-labs/master/data/text-emotion.csv", "text-emotion.csv")
knitr::opts_chunk$set(echo = TRUE)
read.csv('text-emotion.csv')
text_emotion <- read.csv('text-emotion.csv')
text_emotion <- read.csv('text-emotion.csv')
head(text_emotion)
length(text_emtion$content)
length(text_emotion$content)
apply(text_emotion, 4, length)
apply(text_emotion$content, 1, length)
text_emotion$content
apply(text_emotion$content, 2, length)
apply(text_emotion$content, 1, length)
length(text_emotion$content)
sapply(text_emotion$content, 1, length)
sapply(text_emotion$content, length)
length(text_emotion$content[1])
nchar(text_emotion$content[1])
?nchar
??numchar
text_emotion$content[1]
text_emotion$content[[1]]
nchar(text_emotion$content[[1]])
nchar(text_emotion$content[[1]])
sapply(text_emotion$content, nchar)
nchar(text_emotion$content)
typeof(text_emotion$content)
as.character(text_emotion$content)
nchar(as.character(text_emotion$content))
nchars <- nchar(as.character(text_emotion$content))
head(nchars)
summary(nchars)
hist(nchars)
?hist
hist(nchars, breaks = seq(0, 150, 5))
hist(nchars, breaks = seq(0, 160, 5))
hist(nchars, breaks = c(1, 5))
hist(nchars, breaks = 10)
hist(nchars, breaks = 20)
hist(nchars, breaks = seq(0, 150, 5))
hist(nchars, breaks = seq(0, max(nchars), 5))
hist(nchars, breaks = seq(0, max(nchars), 2))
hist(nchars, breaks = seq(0, 170, 5))
hist(nchars, breaks = seq(1, 17`, 5))
hist(nchars, breaks = seq(1, 171, 5))
hist(nchars, breaks = seq(1, 171, 5))
hist(nchars, breaks = seq(1, 171, 5))
nchars == 0
sum(nchars == 0)
# any tweets with 0 characters
any(nchars == 0)
any(nchars == 1)
sum(nchars == 1)
nchars[nchars == 1]
text_emotions$content[nchars == 1]
# content of one-character tweets
text_emotions$content[nchars == 1]
text_emotion$content[nchars == 1]
as.character(text_emotion$content)[nchars == 1]
text_emotion$content[1]
text_emotion$content[1:5]
text_emotion$content[nchars == 1]
which(nchars == 1)
max(nchars)
text_emotion$content[which.max(nchars)]
# location of longest tweet
which.max(nchars)
nchar(text_emtion$author) <= 15
nchar(text_emotion$author) <= 15
nchar(as.character(text_emotion$author)) <= 15
all(nchar(as.character(text_emotion$author)) <= 15)
?all
string_detect(usernames, "\\w+")
library(stringr)
# usernames contain alphanumeric characters and unerscores
usernames <- as.character(text_emotion$author)
string_detect(usernames, "\\w+")
str_detect(usernames, "\\w+")
all(str_detect(usernames, "\\w+"))
str_detect("- 23497", "\\w+")
str_extract("- 23497", "\\w+")
all(str_detect(usernames, "^\\w+."))
str_extract("- 23497", "^\\w+$")
str_detect("- 23497", "^\\w+$")
str_detect("23497", "^\\w+$")
all(str_detect(usernames, "^\\w+$"))
str_detect("23497", "^[\\w+]$")
str_detect("23497", "^[\\w]+$")
all(str_detect(usernames, "^[\\w_]+$"))
str_detect(" 23497", "^[\\w ]+$")
all(str_detect(usernames, "^[\\w_0-9]+$"))
str_detect("_23497", "^[\\w ]+$")
str_detect("%23497", "^[\\w ]+$")
str_detect("_23497", "^[\\w ]+$")
str_detect("_23497", "^[\\w_]+$")
str_detect("_23497", "\\W")
str_detect("%23497", "\\W")
str_detect("%23497", "\\w")
str_detect("%23497", "\\w+")
str_detect("%23497", "^\\w+$")
str_detect("23497", "^\\w+$")
all(str_detect(usernames, "^[a-z0=9_]+$"))
all(str_detect(usernames, "^[\\w_]+$"))
str_extract(usernames, "\\W")
str_extract(usernames, "\\W+")
str_view(usernames, "\\W+")
str_detect("test", "\\W")
str_detect("test", "\\W+")
str_view(usernames, "\\W")
str_subset(usernames, "\\W")
str_subset(usernames, "\\W+")
str_subset(usernames, "\\W")
usernames <- as.character(text_emotion$author)
all(nchar(usernames)) <= 15)
# usernames are not longer than 15 characters
all(nchar(usernames)) <= 15)
# usernames are not longer than 15 characters
all(nchar(usernames) <= 15)
all(str_detect(usernames, "^[\\w]+$"))
nchar(min(usernames))
min(usernames)
min(nchar(usernames))
usernames[which.min(nchar(usernames))]
usernames[min(nchar(usernames))]
which.
# number of characters of the shortest usernames
min(nchar(usernames))
# names of the authors with shortest usernames
usernames[which.min(nchar(usernames))]
sum(str_detect(usernames, "\\^"))
sum(str_detect(usernames, "\\^+"))
sum(str_detect(usernames, "\\^"))
sum(str_detect(usernames, "^"))
sum(str_detect(usernames, "\\^"))
str_count(usernames, "\\w")
str_count(usernames, "\\^")
tweets <- as.character(text_emotion$content)
nchars <- nchar(tweets)
# number of characters in the tweet contents
nchars <- nchar(tweets)
head(nchars)
# content of one-character tweets
tweets[nchars == 1]
# content of longest tweet
tweets[which.max(nchars)]
sum(str_detect(tweets, "\\^"))
sum(str_detect(tweets, "[\\$\\$\\$]+"))
str_detect("$$$$", "[\\$\\$\\$]+")
str_detect("$$", "[\\$\\$\\$]+")
# how many tweets contain 3 or more consecutive dollar symbols
sum(str_detect(tweets, "[\\$][\\$][\\$]+"))
str_detect("$$", "[\\$][\\$][\\$]+")
str_detect("$$$", "[\\$][\\$][\\$]+")
str_detect("$$$$", "[\\$][\\$][\\$]+")
str_subset(tweets, "[\\$][\\$][\\$]+")
sum(str_detect(tweets, "[aA]"))
length(tweets) - sum(str_detect(tweets, "[aA]"))
str_detect("ha", "[aA]")
# display first 10 elements of the tweeets that do NOT contain the characters "a" or "A"
str_which(tweets, "[aA]")
str_which(tweets, "*[aA]")
str_which(tweets, "?![aA]")
str_which(tweets, "?[aA]")
tweets[str_detect(tweets, "[aA]")]
tweets[!str_detect(tweets, "[aA]")]
head(tweets[!str_detect(tweets, "[aA]")])
head(tweets[!str_detect(tweets, "[aA]")], n = 10)
str_count(tweets, "\\!")
summary(str_count(tweets, "\\!"))
tweets[which.max(str_count(tweets, "\\!"))]
head(text_emotion)
sentiments <- text_emotion$sentiment
typeof(sentiments)
class(sentiments)
levels(sentiments)
table(sentiments)
table(sentiments) / length(sentiments)
length(sentiments)
sum(table(sentiments) / length(sentiments))
table(sentiments) / length(sentiments)
plot(relative_frequencies)
# graph relative frequences (proportions) with a horizontal barplot in decreasing order, including names of sentiment types
relative_frequencies <- table(sentiments) / length(sentiments)
plot(relative_frequencies)
barplot(relative_frequencies, horiz = TRUE)
barplot(sorted(relative_frequencies), horiz = TRUE)
barplot(order(relative_frequencies), horiz = TRUE)
barplot(order(relative_frequencies, decreasing = TRUE), horiz = TRUE)
barplot(order(relative_frequencies, decreasing = TRUE))
barplot(order(relative_frequencies, decreasing = TRUE), names.arg = levels(sentiments))
barplot(relative_frequencies, names.arg = levels(sentiments))
barplot(relative_frequencies, names.arg = relative_frequencies )
barplot(relative_frequencies, names.arg = levels(relative_frequencies))
order(relative_frequencies, decreasing = TRUE)
relative_freuqncies
relative_frequencies
sort(relative_frequencies)
relative_frequencies
sort(relative_frequencies, decreasing = TRUE)
barplot(sort(relative_frequencies, decreasing = TRUE), names.arg = levels(relative_frequencies))
barplot(sorted, names.arg = levels(sorted))
# graph relative frequences (proportions) with a horizontal barplot in decreasing order, including names of sentiment types
relative_frequencies <- table(sentiments) / length(sentiments)
sorted <- sort(relative_frequencies, decreasing = TRUE)
barplot(sorted, names.arg = levels(sorted))
barplot(sorted, names.arg = levels(sorted), horiz = TRUE)
sorted <- sort(relative_frequencies, decreasing = FALSE)
# graph relative frequences (proportions) with a horizontal barplot in decreasing order, including names of sentiment types
relative_frequencies <- table(sentiments) / length(sentiments)
sorted <- sort(relative_frequencies, decreasing = FALSE)
barplot(sorted, names.arg = levels(sorted), horiz = TRUE)
library(dplyr)
group_by(text_emotion, sentiment)
group_by(text_emotion, sentiments)
group_by(text_emotion, sentiment)
group_by(text_emotion, factor(sentiment))
group_by(text_emotion, sentiment)
text_emotion %>% group_by(sentiment) $>$ summarise(avg_length = mean(nchar(as.character(content))))
text_emotion %>% group_by(sentiment) %>% summarise(avg_length = mean(nchar(as.character(content))))
sum(str_detect(tweets, "[\\$]{3}"))
str_detect("#", "#")
str_detect("#234", "#")
str_detect("#r", "#[:alpha]")
str_detect("#r", "#[:alpha:]")
str_detect(tweets, "^#[:alpha:][\\w]+$")
sum(str_detect(tweets, "^#[:alpha:][\\w]+$"))
str_subset(tweets, "^#[:alpha:][\\w]+$"))
str_subset(tweets, "^#[:alpha:][\\w]+$")
str_subset(tweets, "^#\\w$")
str_subset(tweets, "^#\\w+$")
str_subset(tweets, "^#.$")
str_subset(tweets, "^#.+$")
str_subset(tweets, "^#.+ $")
str_subset(tweets, "^#.+$")
str_subset(tweets, "^#.+[ ]$")
str_subset(tweets, "^#.+$")
str_subset(tweets, "^#[:alpha:]\\w+$")
str_subset(tweets, "#[:alpha:]\\w+")
str_extract(tweets, "#[:alpha:]\\w+")
str_extract_all(tweets, "#[:alpha:]\\w+")
str_extract(tweets, "#[:alpha:]\\w+")
# number of valid hashtags in the tweet contents
sum(str_detect(tweets, "#[:alpha:]\\w+"))
str_extract(tweets, "#[:alpha:]\\w+")
# number of valid hashtags in the tweet contents
sum(str_detect(tweets, "#[:alpha:]\\w+"))
#str_extract(tweets, "#[:alpha:]\\w+")
str_count(tweets, "#[:alpha:]\\w+")
hashtag_counts <- str_count(tweets, "#[:alpha:]\\w+")
levels(hashtag_counts)
# display frequencies and make barplot of counts
hashtag_counts <- str_count(tweets, "#[:alpha:]\\w+")
levels(hashtag_counts)
levels(as.factor(hashtag_counts))
table(as.factor(hashtag_counts))
barplot(table(as.factor(hashtag_counts)))
# display frequencies and make barplot of counts
hashtag_counts <- str_count(tweets, "#[:alpha:]\\w+")
barplot(table(as.factor(hashtag_counts)))
mean(nchar(str_extract(tweets, "#[:alpha:]\\w+")))
mean(nchar(str_extract_all(tweets, "#[:alpha:]\\w+")))
(str_extract_all(tweets, "#[:alpha:]\\w+"))
str_extract_all(tweets, "#[:alpha:]\\w+")
head(str_extract_all(tweets, "#[:alpha:]\\w+"))
mean(str_extract_all(tweets, "#[:alpha:]\\w+"))
nchar(str_extract_all(tweets, "#[:alpha:]\\w+"))
str_extract_all(tweets, "#[:alpha:]\\w+")
nchar(str_extract_all(tweets, "#[:alpha:]\\w+"))
mean(nchar(str_extract_all(tweets, "#[:alpha:]\\w+")))
mode(nchar(str_extract_all(tweets, "#[:alpha:]\\w+")))
summary(nchar(str_extract_all(tweets, "#[:alpha:]\\w+")))
mean(nchar(str_extract_all(tweets, "#[:alpha:]\\w+")), na.rm = TRUE)
mean(nchar(str_extract_all(tweets, "#[:alpha:]\\w+"), na.rm = TRUE), na.rm = TRUE)
# average length of hashtags
mean(nchar(str_extract_all(tweets, "#[:alpha:]\\w+")), na.rm = TRUE)
sum(str_count(tweets, "#[:alpha:]\\w+"))
hashtag_counts <- sum(str_count(tweets, "#[:alpha:]\\w+"))
barplot(table(as.factor(hashtag_counts)))
# number of valid hashtags in the tweet contents
hashtag_counts <- str_count(tweets, "#[:alpha:]\\w+")
sum(hashtag_counts)
# display frequencies and make barplot of counts
hashtag_counts <- str_count(tweets, "#[:alpha:]\\w+")
barplot(table(as.factor(hashtag_counts)))
# display frequencies and make barplot of counts
barplot(table(as.factor(hashtag_counts)))
hashtag_counts
nchar(str_extract_all(tweets, "#[:alpha:]\\w+"))
str_extract_all(tweets, "#[:alpha:]\\w+")
str_extract(tweets, "#[:alpha:]\\w+")
#str_extract(tweets, "#[:alpha:]\\w+")
tweets[23]
str_length("hi")
str_length(str_extract_all(tweets, "#[:alpha:]\\w+"))
str_length(as.characterstr_extract_all(tweets, "#[:alpha:]\\w+"))
str_length(as.character(str_extract_all(tweets, "#[:alpha:]\\w+")))
typeof(str_extract_all(tweets, "#[:alpha:]\\w+"))
as.vector(str_extract_all(tweets, "#[:alpha:]\\w+"))
str_detect(tweets, "omg|OMG")
sum(str_detect(tweets, "omg|OMG"))
str_detect(tweets, "omg|OMG")
str_extract(tweets, "omg|OMG")
str_subset(tweets, "omg|OMG")
str_subset(tweets, " omg|OMG")
str_subset(tweets, " omg|OMG[ ]|$")
str_subset(tweets, " omg|OMG |$")
str_subset(tweets, " omg|OMG ")
str_subset(tweets, " omg|OMG |\\b")
str_subset(tweets, "\\<omg|OMG\\>)
str_subset(tweets, "\\<omg|OMG\\>"")
str_subset(tweets, "\\<omg|OMG\\>")
str_subset(tweets, "\\<omg|OMG\\>")
str_detect("omg ", "\\<omg|OMG\\>")
str_detect("omg ", "omg|OMG")
str_detect("omg. ", "omg|OMG")
str_detect("omg. ", "\\<omg|OMG")
str_detect("omg. ", "\\<omg|OMG\\>")
knitr::opts_chunk$set(echo = TRUE)
str_detect("#FF0000", "#[0-9]")
str_detect("#FF0000", "#[0-9A-F]")
str_detect("#FF0000", "#[0-9A-Fa-f]{6}")
getwd()
install.packages("devtools")
devtools::install_github("stat133-sp19/hw-stat133-rachelli429/binomial", build_vignettes = TRUE)
knitr::opts_chunk$set(echo = TRUE)
install.packages("devtools")
install.packages("devtools")
knitr::opts_chunk$set(echo = TRUE)
devtools::install_github("stat133-sp19/hw-stat133-rachelli429/binomial", build_vignettes = TRUE)
rm(list = ls())
install.packages("devtools")
install.packages("devtools")
install.packages("devtools")
install.packages("devtools")
knitr::opts_chunk$set(echo = TRUE)
devtools::install_github("stat133-sp19/hw-stat133-rachelli429/binomial", build_vignettes = TRUE)
devtools::install_github("stat133-sp19/hw-stat133-rachelli429/binomial", build_vignettes = TRUE, force = TRUE)
remove.packages("binomial")
devtools::install_github("stat133-sp19/hw-stat133-rachelli429/binomial", build_vignettes = TRUE)
bin_choose(5, 2)
bin_choose(5, 2)
library(binomial)
bin_choose(5, 2)
bin_probability(2, 5, 0.5)
dis1 <- bin_distribution(5, 0.5)
dis1 <- bin_distribution(trials = 5, prob = 0.5)
dis1
plot(dis1)
dis2
dis2 <- bin_cumulative(trials = 5, prob = 0.5)
dis2
plot(dis2)
bin1 <- bin_variable(trials = 10, p = 0.3)
bin1
summary(bin1)
bin_mean(10, 0.3)
bin_variance(10, 0.3)
bin_mode(10, 0.3)
bin_mode(7, 0.5)
bin_skewness(10, 0.3)
bin_kurtosis(10, 0.3)
devtools::document()
devtools
library(devtools)
devtools::document()
setwd("~/Desktop/stat-133/hw-stat133/binomial")
devtools::document()
devtools::check_man()
devtools::test()
devtools::build_vignettes()
devtools::document()
devtools::check_man()
devtools::test()
devtools::build_vignettes()
devtools::build()
devtools::install()
